{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import argparse, sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Layer, Dense, Input, Dropout, Conv2D, BatchNormalization, Activation, Flatten, Concatenate, MaxPooling2D\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "\n",
    "from scipy import stats\n",
    "import scipy\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed_value= 12345\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(\"Keras version: \", keras.__version__)\n",
    "\n",
    "#Define hyperparameters\n",
    "seq_length = 60\n",
    "n_filters = 10\n",
    "filter_size = 3\n",
    "n_layers = 4\n",
    "n_nodes = 40\n",
    "dropout_rate = 0.25 \n",
    "\n",
    "#Define Pepper model\n",
    "class CustomLossLayer(layers.Layer):\n",
    "    def __init__(self, n_proteins_layer, n_runs_layer, **kwargs):\n",
    "\n",
    "        super(CustomLossLayer, self).__init__()\n",
    "        self.n_proteins_layer = n_proteins_layer\n",
    "        self.n_runs_layer = n_runs_layer\n",
    "\n",
    "        #Define alpha variables that are trainable\n",
    "        if n_proteins_layer == 1584 and n_runs_layer == 1:\n",
    "            self.alphas = tf.Variable(alpha_initial_train, \n",
    "                                  trainable = True, \n",
    "                                  dtype = 'float32')\n",
    "        else:\n",
    "\n",
    "            init_values = np.random.rand(1527, \n",
    "                                         96) \n",
    "            self.alphas = tf.Variable(init_values, \n",
    "                                      trainable = True, \n",
    "                                      dtype = 'float32')\n",
    "\n",
    "    def get_vars(self):\n",
    "        return self.alphas\n",
    "\n",
    "    def peptide_loss(self, y_true, y_pred):\n",
    "\n",
    "        #Define all inputs\n",
    "        c_pred = K.abs(y_pred)\n",
    "        c_pred = tf.reshape(c_pred,[-1]) #this is very important for correctness of calculation\n",
    "        q_input = y_true[:, :-1] #dimension (batch_size, K)\n",
    "        label_input = y_true[:, -1] #dimension (batch_size, 1)\n",
    "        label_input = tf.cast(label_input, tf.int32)\n",
    "\n",
    "        #Exclude missing intensities in pairwise distance calculation\n",
    "        zero_peptides = K.not_equal(q_input, K.constant(0))\n",
    "        zero_peptides = K.cast(zero_peptides, K.floatx())\n",
    "\n",
    "        #Exclude peptides with 0 coefficients in pairwise distance calculation\n",
    "        zero_coeffs = K.not_equal(c_pred, K.constant(0))\n",
    "        zero_coeffs = K.cast(zero_coeffs, K.floatx())\n",
    "        zero_coeffs = tf.expand_dims(zero_coeffs, 1)\n",
    "\n",
    "        #Find the corresponding alpha value for each peptide\n",
    "        corresponding_protein_abundances = tf.gather(K.abs(self.alphas), label_input, axis = 0)\n",
    "\n",
    "        #Calculate adjusted abundances\n",
    "        c_pred = tf.expand_dims(c_pred, 1)\n",
    "        adjusted_abundances = c_pred * corresponding_protein_abundances\n",
    "\n",
    "        #Calculate the difference values\n",
    "        differences = q_input - adjusted_abundances\n",
    "        differences = differences * zero_peptides * zero_coeffs\n",
    "        differences = K.square(differences)\n",
    "\n",
    "        #Return the mean loss\n",
    "        total_loss = K.sum(differences)\n",
    "        all_runs = K.sum(zero_peptides * zero_coeffs)\n",
    "        return total_loss / all_runs\n",
    "\n",
    "    #We add the loss to the final model loss\n",
    "    def call(self, y_true, y_pred):\n",
    "        self.add_loss(self.peptide_loss(y_true, y_pred))\n",
    "        return y_pred\n",
    "\n",
    "#Define alpha-based peptide loss function\n",
    "def peptide_loss(x, c_pred, alphas):\n",
    "\n",
    "    c_pred = K.abs(c_pred)\n",
    "\n",
    "    #q_input is the intensity values from the experiment\n",
    "    q_input = x[:, :-1] #dimension (batch_size, K)\n",
    "    #label_input is the protein labels for each peptide\n",
    "    label_input = x[:, -1] #dimension (batch_size, 1)\n",
    "    label_input = tf.cast(label_input, tf.int32)\n",
    "\n",
    "    c_pred = tf.reshape(c_pred,[-1]) #this is very important for correctness of calculation\n",
    "\n",
    "    #Exclude missing intensities in pairwise distance calculation\n",
    "    zero_peptides = K.not_equal(q_input, K.constant(0))\n",
    "    zero_peptides = K.cast(zero_peptides, K.floatx())\n",
    "\n",
    "    #Exclude peptides with 0 coefficients in pairwise distance calculation\n",
    "    zero_coeffs = K.not_equal(c_pred, K.constant(0))\n",
    "    zero_coeffs = K.cast(zero_coeffs, K.floatx())\n",
    "    zero_coeffs = tf.expand_dims(zero_coeffs, 1)\n",
    "\n",
    "    #Find the corresponding alpha value for each peptide\n",
    "    corresponding_protein_abundances = tf.gather(K.abs(alphas), label_input, axis = 0)\n",
    "\n",
    "    #Calculate the differences\n",
    "    c_pred = tf.expand_dims(c_pred, 1)\n",
    "    adjusted_abundances = c_pred * corresponding_protein_abundances\n",
    "    differences = q_input - adjusted_abundances\n",
    "    differences = differences * zero_peptides * zero_coeffs\n",
    "    differences = K.square(differences)\n",
    "\n",
    "    #Record final average loss\n",
    "    total_loss = K.sum(differences)\n",
    "    all_runs = K.sum(zero_peptides * zero_coeffs)\n",
    "    return total_loss / all_runs\n",
    "\n",
    "#Define model\n",
    "def define_model(n_proteins_layer, n_runs_layer):\n",
    "\n",
    "    #Define custom absolute valued activation function\n",
    "    def absActivation(x) :\n",
    "        activated_x = K.abs(x)\n",
    "        return activated_x\n",
    "\n",
    "    #Define network\n",
    "    inputs =  Input(shape=(seq_length, 20, 1), name = 'sequence')\n",
    "    inputs_charge = Input(shape=(6,), name = 'charge')\n",
    "    inputs_label = Input(shape=(n_runs_layer + 1,), name = 'y_true')\n",
    "\n",
    "    #Define convolutional layers\n",
    "    x = Conv2D(n_filters, kernel_size=(filter_size, 20), activation=\"relu\", input_shape=(seq_length, 20, 1))(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 1))(x)\n",
    "    sequence_representation = Flatten()(x)\n",
    "    sequence_representation = Dropout(dropout_rate)(sequence_representation)\n",
    "\n",
    "    #Second input is the one-hot encoded charge states\n",
    "    concatenated_representation = Concatenate()([sequence_representation, inputs_charge])\n",
    "    x = concatenated_representation\n",
    "\n",
    "    #Define dense layers\n",
    "    for n in range(n_layers):\n",
    "        x = Dense(n_nodes, activation=\"relu\")(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    output = Dense(1, activation=absActivation)(x) #predict peptide coefficients\n",
    "\n",
    "    #Define model with custom layer\n",
    "    my_custom_layer = CustomLossLayer(n_proteins_layer, n_runs_layer)(inputs_label, output) # here can also initialize those var1, var2\n",
    "    model = Model(inputs = [inputs, inputs_charge, inputs_label], outputs = my_custom_layer)\n",
    "    model.summary()\n",
    "\n",
    "    #Compile the model\n",
    "    opt = tf.optimizers.Adam(learning_rate = 1e-3, clipnorm = 1)\n",
    "    model.compile(optimizer=opt)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#Load pretrained model\n",
    "pretrained_model = define_model(1527, 96)   \n",
    "pretrained_model.load_weights('../trained_models/2019_guo_nci60/2019_guo_nci60_Coefficient_Predictor_Model.h5')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Generate sequences with substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode the original sequences\n",
    "#Function to encode sequences to int\n",
    "aminoacids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "def sequence_to_int(sequence, char_to_int):\n",
    "    integer_encoded = []\n",
    "    \n",
    "    for char in sequence:\n",
    "        if char in aminoacids:\n",
    "            integer_encoded.append(char_to_int[char] + 1)\n",
    "        else:\n",
    "            integer_encoded.append(0)\n",
    "    return integer_encoded\n",
    "\n",
    "#Function to encode sequences to one hot\n",
    "def sequence_to_onehot(sequence, char_to_int):\n",
    "\n",
    "    onehot_encoded = list()\n",
    "    \n",
    "    #First convert to int\n",
    "    integer_encoded = sequence_to_int(sequence, char_to_int)\n",
    "    \n",
    "    #Then, one hot encode each int\n",
    "    for value in integer_encoded:\n",
    "        letter = [0 for _ in range(len(aminoacids))]\n",
    "        if value != 0:\n",
    "            letter[value - 1] = 1\n",
    "            onehot_encoded.append(letter)\n",
    "        else:\n",
    "            onehot_encoded.append(letter) \n",
    "    #Flatten onehot_encoded\n",
    "    onehot_encoded = np.array(onehot_encoded).flatten()\n",
    "    \n",
    "    #Convert to dataframe\n",
    "    indices = []\n",
    "    for s in range(len(sequence)):\n",
    "        sub_indices = [(str(s) + '_' + c) for c in aminoacids]\n",
    "        indices.extend(sub_indices)\n",
    "    onehot_encoded_df = pd.DataFrame(onehot_encoded, index = indices)\n",
    "    \n",
    "    return onehot_encoded_df\n",
    "\n",
    "\n",
    "def generateSequences(all_peptide_sequences, charges, sample_size):\n",
    "    \n",
    "    random.seed(12345)\n",
    "    \n",
    "    #Randomly sample from the sequences\n",
    "    all_peptide_sequences = random.sample(list(all_peptide_sequences), \n",
    "                                          sample_size)\n",
    "    \n",
    "    \n",
    "    aminoacids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    #Make sure all sequences have the same length\n",
    "    imputed_peptide_sequences = []\n",
    "    max_length = 60\n",
    "    for i in range(len(all_peptide_sequences)):\n",
    "        s = all_peptide_sequences[i]\n",
    "        new_s = s + ' ' * (max_length - len(s))\n",
    "        imputed_peptide_sequences.append(new_s)\n",
    "    imputed_peptide_sequences  \n",
    "\n",
    "    #Define a mapping of chars to integers\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(aminoacids))\n",
    "\n",
    "    #Now encode each sequence\n",
    "    all_peptide_sequences_onehot = []\n",
    "    for i in range(len(imputed_peptide_sequences)):\n",
    "        sequence = imputed_peptide_sequences[i]\n",
    "        onehot = sequence_to_onehot(sequence, char_to_int)\n",
    "        all_peptide_sequences_onehot.append(onehot)\n",
    "\n",
    "    onehot_encoded_df = pd.concat(all_peptide_sequences_onehot, axis = 1).T\n",
    "    print(\"onehot_encoded_df \", onehot_encoded_df.shape)\n",
    "    \n",
    "    #Modify every letter of every sequence\n",
    "    aminoacids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    all_sequence_pairs = []\n",
    "    all_sequence_pairs_onehot_encoded = []\n",
    "    all_charge_pairs = []\n",
    "    \n",
    "    for s in range(len(all_peptide_sequences)):\n",
    "        seq = all_peptide_sequences[s]\n",
    "        #print(\"-------\", seq)\n",
    "        for p in range(len(seq)):\n",
    "            for a in aminoacids:\n",
    "                new_seq = seq[0:p] + a + seq[p + 1:] \n",
    "\n",
    "                #Record results\n",
    "                difference_char = seq[p] + '-' + a\n",
    "                difference_pos_start = p\n",
    "                difference_pos_end = len(seq) - p - 1\n",
    "                pair = [seq, new_seq, difference_char, difference_pos_start, difference_pos_end]\n",
    "                all_sequence_pairs.append(pair)\n",
    "\n",
    "                #Record onehot-encoded results\n",
    "                onehot_seq1 = onehot_encoded_df.iloc[s, :].values.ravel()\n",
    "                onehot_seq2 = copy.copy(onehot_seq1)\n",
    "                onehot_seq2[(p*20):((p+1)*20)] = 0\n",
    "                onehot_seq2[(p*20) + aminoacids.index(a)] = 1\n",
    "\n",
    "                pair = np.concatenate([onehot_seq1, onehot_seq2])\n",
    "                all_sequence_pairs_onehot_encoded.append(pair)\n",
    "\n",
    "                all_charge_pairs.append([charges[s], charges[s]])\n",
    "\n",
    "    #Convert to dataframe\n",
    "    all_sequence_pairs = pd.DataFrame(all_sequence_pairs, columns = ['Pair1', 'Pair2', 'Substitution', 'Start Position', 'End Position'])\n",
    "    return all_sequence_pairs, all_sequence_pairs_onehot_encoded, all_charge_pairs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read the peptide sequences from dataset 1\n",
    "sequence_df = pd.read_csv('../preprocess_datasets/preprocessed_datasets/2019_guo_nci60_formatted_peptide_quants.tsv',\n",
    "                          sep = '\\t', index_col = 0)\n",
    "\n",
    "#Modify the sequences to exclude modifications\n",
    "peptide_sequences = sequence_df['Peptide'].values\n",
    "peptide_sequences = [s.replace('(UniMod:4)', '') for s in peptide_sequences]\n",
    "peptide_sequences = [s.replace('(UniMod:35)', '') for s in peptide_sequences]\n",
    "\n",
    "charges = sequence_df[['Charge 1', 'Charge 2', 'Charge 3', 'Charge 4', 'Charge 5', 'Charge 6']].values\n",
    "print(\"Number of sequences \", len(peptide_sequences))\n",
    "\n",
    "peptide_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = generateSequences(peptide_sequences, charges, sample_size = 5000)\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode simulated sequences\n",
    "all_sequence_pairs = results[0] \n",
    "all_sequence_pairs_onehot_encoded = results[1] \n",
    "all_sequence_pairs_charges = np.array(results[2])\n",
    "\n",
    "pair1_sequences = np.array(all_sequence_pairs_onehot_encoded)[:,:60 * 20]\n",
    "pair1_sequences = np.array(pair1_sequences).reshape((pair1_sequences.shape[0], 60, 20))\n",
    "pair1_sequences = np.expand_dims(pair1_sequences, axis=3)\n",
    "\n",
    "pair1_charges = all_sequence_pairs_charges[:, 0]\n",
    "\n",
    "pair1_coefficients = pretrained_model.predict([pair1_sequences, pair1_charges,\n",
    "                                               np.ones((pair1_sequences.shape[0],97))]).ravel()\n",
    "pair1_coefficients = np.abs(pair1_coefficients)\n",
    "pair1_coefficients = pd.DataFrame(pair1_coefficients)\n",
    "print(\"Pair 1 coefficients \", pair1_coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode simulated sequences\n",
    "pair2_sequences =  np.array(all_sequence_pairs_onehot_encoded)[:, -1 * 60 * 20:]\n",
    "pair2_sequences = np.array(pair2_sequences).reshape((pair2_sequences.shape[0], 60, 20))\n",
    "pair2_sequences = np.expand_dims(pair2_sequences, axis=3)\n",
    "\n",
    "\n",
    "pair2_charges = all_sequence_pairs_charges[:, 1]\n",
    "\n",
    "pair2_coefficients = pretrained_model.predict([pair2_sequences, pair2_charges,\n",
    "                                               np.ones((pair2_sequences.shape[0],97))]).ravel()\n",
    "pair2_coefficients = np.abs(pair2_coefficients)\n",
    "pair2_coefficients = pd.DataFrame(pair2_coefficients)\n",
    "print(\"Pair 2 coefficients \", pair2_coefficients)\n",
    "\n",
    "#Calculate coefficient differences\n",
    "coeff_differences = pair1_coefficients.values.ravel() - pair2_coefficients.values.ravel() \n",
    "coeff_differences = pd.DataFrame(coeff_differences, index = all_sequence_pairs.index, columns = ['Coefficient Difference'])\n",
    "coeff_differences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Create substitution heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sequence_pairs_new = pd.concat([all_sequence_pairs, coeff_differences], axis = 1)\n",
    "summary_df = all_sequence_pairs_new.groupby('Substitution')['Coefficient Difference'].median()\n",
    "summary_df = pd.DataFrame(summary_df.values.reshape((20, 20)).T, \n",
    "                          index = list('ACDEFGHIKLMNPQRSTVWY'),\n",
    "                          columns = list('ACDEFGHIKLMNPQRSTVWY'))\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Create plots\n",
    "SMALL_SIZE = 70\n",
    "MEDIUM_SIZE = 80\n",
    "BIGGER_SIZE = 90\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "row_linkage = hierarchy.linkage(\n",
    "    distance.pdist(((summary_df.values.reshape((20, 20)).T))), method='average')\n",
    "\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list('mycmap', ['#22a6b3', '#ffffff', '#eb4d4b'])\n",
    "\n",
    "g = sns.clustermap(summary_df, cmap = cmap,\n",
    "                   row_linkage=row_linkage, col_linkage=row_linkage,\n",
    "                   figsize = (50, 50), \n",
    "                   metric = 'euclidean', method = 'average',\n",
    "                   vmin = -0.05, vmax = 0.05, \n",
    "                   linewidth = 10, linecolor = '#535c68', square=True)\n",
    "\n",
    "for a in g.ax_row_dendrogram.collections:\n",
    "    a.set_linewidth(5)\n",
    "\n",
    "for a in g.ax_col_dendrogram.collections:\n",
    "    a.set_linewidth(5)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
